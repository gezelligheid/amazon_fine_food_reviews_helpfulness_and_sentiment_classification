{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# adapted from:\n",
    "# - https://towardsdatascience.com/scikit-learn-for-text-analysis-of-amazon-fine-food-reviews-ea3b232c2c1b\n",
    "# - https://github.com/shshnk158/Amazon-Fine-Food-Reviews-Prediciton/blob/master/Logistic%20Regression%20Assignment.ipynb\n",
    "\n",
    "import inline as inline\n",
    "# %matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "define required funtion(s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import re\n",
    "def process(text):\n",
    "        \"\"\" Input: A series of senctences [w1,w2,w3,....]\"\"\"\n",
    "\n",
    "        temp_1 = []\n",
    "        snow = nltk.stem.SnowballStemmer('english')\n",
    "        i = 0\n",
    "        n = len(text)\n",
    "        for sentence in text:\n",
    "            sentence = sentence.lower()\n",
    "            cleanr = re.compile('<.*?>')\n",
    "            sentence = re.sub(cleanr, ' ', sentence)  # Removing HTML tags\n",
    "            sentence = re.sub(r'[?|!|\\'|\"|#]', r'', sentence)\n",
    "            sentence = re.sub(r'[.|,|)|(|\\|/]', r' ', sentence)  # Removing Punctuations\n",
    "\n",
    "            words = [snow.stem(word) for word in sentence.split() if\n",
    "                     word not in stopwords.words('english')]  # Splitting the words\n",
    "            temp_1.append(words)  # and doing stemming\n",
    "            print(\"{0:.2f} %\".format((i / n) * 100), end='\\r')\n",
    "            i = i + 1\n",
    "\n",
    "        text = temp_1\n",
    "\n",
    "        sent = []\n",
    "        for row in text:\n",
    "            seq = ''\n",
    "            for word in row:\n",
    "                seq = seq + ' ' + word\n",
    "            sent.append(seq)\n",
    "\n",
    "        text = sent\n",
    "\n",
    "        \"\"\" Output: Series of sentences after processing \"\"\"\n",
    "        return text\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/Reviews.csv')\n",
    "# df.head()\n",
    "# df.dropna(inplace=True)\n",
    "# keep only reviews that are strongly helpful or unhelpful\n",
    "# mean_helpfulness = df['HelpfulnessNumerator'].sum()/df['HelpfulnessDenominator'].sum()\n",
    "# df = df[(df['HelpfulnessDenominator'] > 6)\n",
    "#         & ((df['HelpfulnessNumerator']/df['HelpfulnessDenominator'] < 0.33)\n",
    "#              | (df['HelpfulnessNumerator']/df['HelpfulnessDenominator'] > mean_helpfulness))]\n",
    "# adding helpfulness indicating column of a binary attribute where 1 implies helpful, 0 otherwise\n",
    "# df['Helpful'] = np.where(df['HelpfulnessNumerator']/df['HelpfulnessDenominator'] > mean_helpfulness, 1, 0)\n",
    "# filtered_data['Helpful'] = np.where(filtered_data['HelpfulnessNumerator']/filtered_data['HelpfulnessDenominator'] > mean_helpfulness, 1, 0)\n",
    "# df.head()\n",
    "# filtered_data.head()\n",
    "# inspect the amount of negative/unhelpful examples\n",
    "# print(len(df[df['Helpful'] == 0].index))\n",
    "\n",
    "X_train = pd.read_csv('../data/X_train.csv')\n",
    "X_train = X_train['Text']\n",
    "X_test = pd.read_csv('../data/X_test.csv')\n",
    "X_test = X_test['Text']\n",
    "y_train = pd.read_csv('../data/y_train.csv')\n",
    "y_train = y_train['Helpful']\n",
    "y_test = pd.read_csv('../data/y_test.csv')\n",
    "y_test = y_test['Helpful']"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Helpful'], random_state = 0)\n",
    "# # an index that exists in the pruned dataset\n",
    "# print('X_train first entry: \\n\\n', X_train.head(1))#hdenom possible data points 6: 5:36527 4:135201 3:25288\n",
    "# print('\\n\\nX_train shape: ', X_train.shape)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# X_train = process(X_train)\n",
    "# X_test = process(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "using ngrams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['00',\n 'air tight contain',\n 'aspartam price amazon',\n 'bi',\n 'buy natur',\n 'chick pea',\n 'complaint wish',\n 'cuppa black green',\n 'discontinu upset',\n 'eat soy jerki',\n 'extra chore',\n 'flavor good',\n 'fyi',\n 'grate find',\n 'histori mani',\n 'intens casual drinker',\n 'leafi green',\n 'long long',\n 'meal cat',\n 'muir',\n 'ok edibl even',\n 'packag open',\n 'potenti organ toxic',\n 'puppi love new',\n 'regular white',\n 'say give',\n 'size pea',\n 'state mind easi',\n 'syrup general good',\n 'think anyon',\n 'turkey can recal',\n 'vitamin calcium iron',\n 'without doubt']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(min_df = 5, ngram_range = (1,3),max_features=2**16).fit(X_train)\n",
    "vect.get_feature_names()[::2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "65536"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<23885x65536 sparse matrix of type '<class 'numpy.int64'>'\n\twith 1969652 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8096714342991335\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0.65987934, 0.34012066],\n       [0.04053647, 0.95946353]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "confusion_matrix(y_test,predictions,normalize='true')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs: \n",
      "['america' 'might well' 'didnt work' 'drown' 'flavor great' 'cancel' '55'\n",
      " 'disgust' 'ruin' 'your go']\n",
      "\n",
      "Largest Coefs: \n",
      "['beauti' 'excel' 'satisfi' 'pricey' 'realli good' 'delici' 'outstand'\n",
      " 'tender' 'beat' 'discontinu']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.9466171003717472"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "print('Smallest Coefs: \\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}\\n'.format(feature_names[sorted_coef_index[:-11:-1]]))\n",
    "f1_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "export the train and test set for easy reuse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import csv\n",
    "# with open('../data/X_train.csv','wb') as result_file:\n",
    "#     wr = csv.writer(result_file,dialect='excel')\n",
    "#     wr.writerows(X_train)\n",
    "#\n",
    "# with open('../data/X_test.csv','wb') as result_file:\n",
    "#     wr = csv.writer(result_file,dialect='excel')\n",
    "#     wr.writerows(X_test)\n",
    "X_train_dict = {'Text': X_train}\n",
    "X_test_dict = {'Text': X_test}\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_dict,columns=['Text'])\n",
    "X_test_df = pd.DataFrame(X_test_dict,columns=['Text'])\n",
    "\n",
    "X_train_df.to_csv(r'../data/X_train.csv', index= False)\n",
    "X_test_df.to_csv(r'../data/X_test.csv', index= False)\n",
    "\n",
    "y_train.to_csv(r'../data/y_train.csv', index=False)\n",
    "y_test.to_csv(r'../data/y_test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}